[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog Test"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Optimal Decision Making\n\n\n\n\n\nBuilding a model to maximize gain for a bank given past loan data\n\n\n\n\n\nMar 21, 2024\n\n\nTristan Meyer\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nPredicting Penguin Species\n\n\n\n\n\nMar 4, 2023\n\n\nTristan Meyer\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "ClassifyingPalmerPenguins.html",
    "href": "ClassifyingPalmerPenguins.html",
    "title": "Explore:",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "ClassifyingPalmerPenguins.html#explore",
    "href": "ClassifyingPalmerPenguins.html#explore",
    "title": "Explore:",
    "section": "Explore:",
    "text": "Explore:\nPlease create two interesting visualizations of the data, and one summary table (e.g. compute the average or median value of some features, by group). Your visualizations and table should:\n\nInclude axis labels and legends.\nHelp you draw conclusions about what features you are going to try using for your model.\nBe accompanied by discussion of what you learned and how you will use it in modeling. Most figures and tables are worth at least one short paragraph of discussion.\n\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ndf1 = train[[\"Sex\",\"Body Mass (g)\", \"Species\"]]\ndf1 = df1.drop(df1[df1[\"Sex\"] == \".\"].index)\ndf1[\"Species\"] = df1[\"Species\"].str.split().str.get(0)\ndf1 = df1.dropna()\n\np1 = sns.boxplot(df1, x = \"Sex\", y = \"Body Mass (g)\", hue = \"Species\").set_title(\"Comparing Penguins' Body Mass and Sex across Species\")\n\n\n\n\n\n\n\n\nDESCRIBE SOMETHING\n\ndf2 = train[[\"Body Mass (g)\", \"Flipper Length (mm)\"]]\ndf2 = df2.dropna()\n\np2 = sns.scatterplot(df2, x = \"Body Mass (g)\", y = \"Flipper Length (mm)\", color = \"blue\").set_title(\"Comparing Flipper Length and Body Mass of Penguins\")\n\n\n\n\n\n\n\n\nDESCRIBE SOMETHING\n\ndf3 = train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island\"]]\ndf3 = df3.dropna()\n\np3 = sns.scatterplot(df3, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue=\"Island\").set_title(\"Comparing Culmen Length and Culmen Depth across Islands\")\n\n\n\n\n\n\n\n\nDESCRIBE SOMETHING\n\ndf4 = train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Sex\"]]\ndf4.groupby(\"Sex\").aggregate('mean')\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\nSex\n\n\n\n\n\n\n\n\n.\n44.500000\n15.700000\n217.000000\n4875.000000\n\n\nFEMALE\n42.551128\n16.371429\n198.067669\n3882.330827\n\n\nMALE\n45.463359\n17.993130\n203.687023\n4521.755725\n\n\n\n\n\n\n\nWrite something"
  },
  {
    "objectID": "ClassifyingPalmerPenguins.html#model",
    "href": "ClassifyingPalmerPenguins.html#model",
    "title": "Explore:",
    "section": "Model:",
    "text": "Model:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns\n\n\n\nPreparing the qualitative columns in the dataset and removing the Species column\nWrite Something\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Stage_Adult, 1 Egg Stage\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nmax_score = 0\nmax_cols = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression(max_iter=10000)\n    LR.fit(X_train[cols], y_train)\n    score = LR.score(X_train[cols], y_train)\n    if score &gt; max_score:\n      max_score = score\n      max_cols = cols\nmax_cols\n\n['Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen',\n 'Culmen Length (mm)',\n 'Culmen Depth (mm)']\n\n\nWrite Something\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\nmax_cols.reverse()\n\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[max_cols], y_train)\nLR.score(X_test[max_cols], y_test)\n\n1.0\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[max_cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test[max_cols], y_test)\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[max_cols])\nC = confusion_matrix(y_test,y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])"
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#abstract",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this post was to create a model that predicts a penguin’s species based on both quantitative and qualitative data. Personally, this post demonstrates my ability to work with and manipulate dataframes in Pandas, create visualizations using seaborn, choose effective features, and build an effective model. My model uses Culmen Depth, Culmen Length and the island of inhabitants of the penguins in the Palmer Penguin data set to predict the species of the penguin. I used a brute force tactic to choose these three features: iterating through every combination of features and fitting a logistic regression to each combination. I compared the mean cross validation scores for each combination and chose the three features with the highest score. I displayed the results of my model on both the test and training data, showing the prediction areas produced by the features."
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#explore",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#explore",
    "title": "Classifying Palmer Penguins",
    "section": "Explore:",
    "text": "Explore:\nPlease create two interesting visualizations of the data, and one summary table (e.g. compute the average or median value of some features, by group). Your visualizations and table should:\n\nInclude axis labels and legends.\nHelp you draw conclusions about what features you are going to try using for your model.\nBe accompanied by discussion of what you learned and how you will use it in modeling. Most figures and tables are worth at least one short paragraph of discussion.\n\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nImporting the Palmer Penguin Dataset.\n\nimport seaborn as sns\n\ndf1 = train[[\"Sex\",\"Body Mass (g)\", \"Species\"]]\ndf1 = df1.drop(df1[df1[\"Sex\"] == \".\"].index)\ndf1[\"Species\"] = df1[\"Species\"].str.split().str.get(0)\ndf1 = df1.dropna()\n\np1 = sns.boxplot(df1, x = \"Sex\", y = \"Body Mass (g)\", hue = \"Species\").set_title(\"Comparing Penguins' Body Mass and Sex across Species\")\n\n\n\n\n\n\n\n\nI created a new data frame with the the Sex, Body Mass, and Species columns. I removed the penguins with the third sex (not male or female) because there were not enough specimens with this sex to make an impact on this visualization. Further, I dropped all of the rows with NA values for any of these features. I then created a boxplot with Sex on the X-axis and Body Mass on the Y-axis with the individual boxplots separated by species. This plot shows that the average body mass of all species is greater for male penguins compared to females. Further, (and important for feature selection) it shows the similarities between body mass between the Chinstrap and Adelie species.\n\ndf2 = train[[\"Body Mass (g)\", \"Flipper Length (mm)\"]]\ndf2 = df2.dropna()\n\np2 = sns.scatterplot(df2, x = \"Body Mass (g)\", y = \"Flipper Length (mm)\", color = \"blue\").set_title(\"Comparing Flipper Length and Body Mass of Penguins\")\n\n\n\n\n\n\n\n\nI created another data frame (dropping the NA values again) including the body mass and flipper length features. I displayed these features using a scatter plot. This visualization shows that there is a positive correlation between body mass and flipper length among all the penguins in the data.\n\ndf3 = train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island\"]]\ndf3 = df3.dropna()\n\np3 = sns.scatterplot(df3, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue=\"Island\").set_title(\"Comparing Culmen Length and Culmen Depth across Islands\")\n\n\n\n\n\n\n\n\nFor my final visualization, I created another data frame that included the culmen length, culmen depth, and island features. This visualization shows that particular ranges of culmen length and depth could be associated with different islands. For example, generally, penguins with culmen depth under 16 mm and culmen length over 43 mm belong to the Biscoe island.\n\ndf4 = train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Sex\"]]\ndf4.groupby(\"Sex\").aggregate('mean')\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\nSex\n\n\n\n\n\n\n\n\n.\n44.500000\n15.700000\n217.000000\n4875.000000\n\n\nFEMALE\n42.551128\n16.371429\n198.067669\n3882.330827\n\n\nMALE\n45.463359\n17.993130\n203.687023\n4521.755725\n\n\n\n\n\n\n\nI created this summary table by grouping the penguins and sex and aggregating the culmen length, culmen depth, flipper length, and body mass features by their mean values. Through this table, the differences in these quantitative values between sexes."
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#model",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#model",
    "title": "Classifying Palmer Penguins",
    "section": "Model:",
    "text": "Model:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nPreparing the qualitative columns in the dataset and removing the Species column. I also removed the other identifying columns for the penguins and removed the third sex (not female or male) because there are a small number of penguins with this sex. I also one hot encoded the categorical feature columns.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Stage_Adult, 1 Egg Stage\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nmax_cv_score = 0\nmax_cols = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression(max_iter=10000)\n    LR.fit(X_train[cols], y_train)\n    cv_scores = cross_val_score(LR,X_train[cols],y_train,cv = 10)\n    cv_score_mean = cv_scores.mean()\n    if cv_score_mean &gt; max_cv_score:\n      max_cv_score = cv_score_mean\n      max_cols = cols\nmax_cols\n\n['Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen',\n 'Culmen Length (mm)',\n 'Culmen Depth (mm)']\n\n\nI split the features into the quantitative columns and the qualitative columns. Next, for every qualitative feature, I iterated through every combination of two quantitative features. For each of these combinations of one qualitative and two quantitative features, I fit a logistic regression model and calculated the mean cross evaluation score over ten folds. I kept track of the greatest mean cross evaluation score and found that Culmen Length, Culmen Depth, and Island performed best.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\nmax_cols.reverse()\n\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[max_cols], y_train)\nLR.score(X_test[max_cols], y_test)\n\n1.0\n\n\nI imported the test data, and fit a logistic regression on the training data with the columns that performed the best in the training data. I then scored the model using the testing data and found that it performed to 100% accuracy.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[max_cols], y_train)\n\n\n\n\n\n\n\n\nThis plot shows the prediction regions for my logistic regression model. As shown in the plot, the model uses culmen length and depth and island of inhabitants to make it’s predictions. The training data is shown in this visualization\n\nplot_regions(LR, X_test[max_cols], y_test)\n\n\n\n\n\n\n\n\nThe plot above uses the test data instead of the training data over the same variables. As shown in the plot, the model performed with one hundred percent accuracy for the test data.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[max_cols])\nC = confusion_matrix(y_test,y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nThe above confusion matrix shows that all of the Adelie penguins were classified as Adelie penguins, all of the Chinstrap penguins were classified as Chinstrap penguins, and all of the Gentoo penguins were classified as Gentoo."
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this post was to create a model that predicts a penguin’s species based on both quantitative and qualitative data. Personally, this post demonstrates my ability to work with and manipulate dataframes in Pandas, create visualizations using seaborn, choose effective features, and build an effective model. My model uses Culmen Depth, Culmen Length and the island of inhabitants of the penguins in the Palmer Penguin data set to predict the species of the penguin. I used a brute force tactic to choose these three features: iterating through every combination of features and fitting a logistic regression to each combination. I compared the mean cross validation scores for each combination and chose the three features with the highest score. I displayed the results of my model on both the test and training data, showing the prediction areas produced by the features."
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#discussion",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion:",
    "text": "Discussion:\nThrough the use of an exhaustive feature search and cross validation, I created a model that uses culmen depth, culmen length, and the island of inhabitants to predict the species of the penguin. My model performed with 99.6 percent accuracy in the training data with a cross validation score of 99.6 percent as well, and performed with one hundred percent accuracy in the test data.\nThrough this process I improved my skills with pandas data frames and fitting linear regression models. I also learned about feature selection. I experimented with multiple tools in the scikit learn database (VarianceThreshold, SelectKBest, etc.), but I didn’t end up using any of these tools for my selection. I learned how to test every combination of features and practiced my skills with cross validation on each of these combinations."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/index.html",
    "href": "posts/OptimalDecisionMaking/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/index.html#math",
    "href": "posts/OptimalDecisionMaking/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html",
    "title": "Optimal Decision Making",
    "section": "",
    "text": "The aim of this post was to create a model to maximize the gain on loans for a bank. This post demonstrates my ability to create visualizations in seaborn, choose featuers for my model, threshold my model, and analyze my model from the point of view of both the loaner and the borrower. My model uses the person’s home ownership status, the percent of their income that the loan is, and the length of their most recent employment to predict whether or not the borrower defaulted on the loan. My model performs at 85 percent accuracy in the test data. I used a brute force tactic to choose these three features: iterating through every combination of features and fitting a logistic regression to each combination. I compared the mean cross validation scores for each combination and chose the three features with the highest score. I found the threshold that maximized gain for the bank by testing a wide range of values. I calculated the gain based on profit maximization by calculating the cost of a false negative and the gain of a true negative. I found that a threshold value of 1.086 maximized the banks profit with an average gain of 333 in the training data and 389 in the testing data."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#abstract",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#abstract",
    "title": "Optimal Decision Making",
    "section": "",
    "text": "The aim of this post was to create a model to maximize the gain on loans for a bank. This post demonstrates my ability to create visualizations in seaborn, choose featuers for my model, threshold my model, and analyze my model from the point of view of both the loaner and the borrower. My model uses the person’s home ownership status, the percent of their income that the loan is, and the length of their most recent employment to predict whether or not the borrower defaulted on the loan. My model performs at 85 percent accuracy in the test data. I used a brute force tactic to choose these three features: iterating through every combination of features and fitting a logistic regression to each combination. I compared the mean cross validation scores for each combination and chose the three features with the highest score. I found the threshold that maximized gain for the bank by testing a wide range of values. I calculated the gain based on profit maximization by calculating the cost of a false negative and the gain of a true negative. I found that a threshold value of 1.086 maximized the banks profit with an average gain of 333 in the training data and 389 in the testing data."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-a-grab-the-data",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-a-grab-the-data",
    "title": "Optimal Decision Making",
    "section": "Part A: Grab the Data",
    "text": "Part A: Grab the Data\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\nMORTGAGE\n8.0\nEDUCATION\nA\n3000\n7.29\n0\n0.02\nN\n17\n\n\n26060\n23\n48000\nRENT\n1.0\nVENTURE\nA\n4325\n5.42\n0\n0.09\nN\n4\n\n\n26061\n22\n60000\nRENT\n0.0\nMEDICAL\nB\n15000\n11.71\n0\n0.25\nN\n4\n\n\n26062\n30\n144000\nMORTGAGE\n12.0\nPERSONAL\nC\n35000\n12.68\n0\n0.24\nN\n8\n\n\n26063\n25\n60000\nRENT\n5.0\nEDUCATION\nA\n21450\n7.29\n1\n0.36\nN\n4\n\n\n\n\n26064 rows × 12 columns\n\n\n\nDownloading the training data"
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-b-exploring-the-data",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-b-exploring-the-data",
    "title": "Optimal Decision Making",
    "section": "Part B: Exploring the Data",
    "text": "Part B: Exploring the Data\n\nimport seaborn as sns\n\ndf1 = df_train[[\"loan_intent\", \"loan_amnt\"]]\ndf1 = df1.dropna()\n\np1 = sns.histplot(data=df1, x=\"loan_amnt\", hue=\"loan_intent\", multiple=\"stack\", bins = 10)\np1.set(xlabel=\"Loan Amount\", title=\"Visualizing Loan Intent over Differing Loan Amounts\")\np1\n\n/Users/tristanmeyer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThis visualization shows a histogram of loan amounts shown over twenty bins separated by loan intent. Shown by the visualization, the majority of loans occur in the $5,000 - $15,000 range. Further, over the range of loan amounts, the proportions of the different loan intents stays relatively steady.\n\ndf2 = df_train[[\"person_age\", \"loan_int_rate\"]]\ndf2 = df2.dropna()\ndf2['cat_age'] = np.select([df2.person_age &lt; 30, df2.person_age &lt; 40, df2.person_age &lt; 50, df2.person_age &lt; 60, df2.person_age &lt; 70,], ['&lt;30', '30-40','40-50','50-60','60-70'], '&gt;70')\ndf2 = df2.groupby(\"cat_age\", sort=False).aggregate('mean')\n\np2 = sns.lineplot(data=df2, x=\"cat_age\", y = \"loan_int_rate\")\np2.set(xlabel =\"Ages\", ylabel = \"Loan Interest Rates (%)\", title =\"Visualization Loan Interest Rates over Different Ages\")\np2\n\n/Users/tristanmeyer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/tristanmeyer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nTo create this visualization, I created a new column that grouped the ages into ten year spans. Then, I aggregated the data frame by the mean values for the different categories in this new column. I then visualized this data over a line plot with the age categories on the x axis and the loan interest rates on the y axis. The graph shows a fluctuation between 11 to 11.1 for the ages up to 70 years old and then a jump to 11.3 for the loans given to people over 70.\n\ndf3 = df_train[[\"person_age\", \"person_income\", \"person_home_ownership\", \"loan_amnt\"]]\ndf3 = df3.dropna()\ndf3.groupby(\"person_home_ownership\", sort=False).aggregate('mean')\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nloan_amnt\n\n\nperson_home_ownership\n\n\n\n\n\n\n\nRENT\n27.532802\n54961.066515\n8843.507973\n\n\nMORTGAGE\n28.005129\n81076.729087\n10562.137462\n\n\nOWN\n27.639462\n57348.641383\n8978.912626\n\n\nOTHER\n27.159091\n78263.238636\n11235.795455\n\n\n\n\n\n\n\nThis summary table shows the difference between the loan receivers age, income, and loan amount over the different home ownership statuses. The table shows that age remains relatively constant for all categories of home ownership. The table also shows an increased income for those with ‘mortgage’ and ‘other’ statuses compared to the ‘rent’ and ‘own’ statuses. The loan amount follows a similar pattern to the income, the ‘mortgage’ and ‘other’ statuses are greater than the ‘rent’ and ‘own’ statuses."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-c-build-a-model",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-c-build-a-model",
    "title": "Optimal Decision Making",
    "section": "Part C: Build a Model",
    "text": "Part C: Build a Model\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndef prepare_data(df):\n  df = df.drop([\"loan_grade\"], axis = 1)\n  df = df.dropna()\n  y = df[\"loan_status\"]\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\nPreparing the qualitative columns in the dataset and removing the loan status and loan grade columns. I also one hot encoded the categorical feature columns.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nall_quant_cols = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\nall_qual_cols = [\"person_home_ownership\",\"loan_intent\",\"cb_person_default_on_file\"]\nmax_cv_score = 0\nmax_cols = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression(max_iter=10000)\n    LR.fit(X_train[cols], y_train)\n    cv_scores = cross_val_score(LR,X_train[cols],y_train,cv = 10)\n    cv_score_mean = cv_scores.mean()\n    if cv_score_mean &gt; max_cv_score:\n      max_cv_score = cv_score_mean\n      max_cols = cols\nmax_cols\n\n['person_home_ownership_MORTGAGE',\n 'person_home_ownership_OTHER',\n 'person_home_ownership_OWN',\n 'person_home_ownership_RENT',\n 'person_emp_length',\n 'loan_percent_income']\n\n\nI split the features into the quantitative columns and the qualitative columns. Next, for every qualitative feature, I iterated through every combination of every possible number of quantitative features. For each of these combinations of one qualitative and different number of quantitative features, I fit a logistic regression model and calculated the mean cross evaluation score over ten folds. I found that a combination between two quantitative features performed just as well as combinations of three and four quantitative features and combinations of five and six quantitative features performed worse. Therefore, with two quantitative features and one qualitative feature the person’s length of employment and the percent income that the loan is performed best as quantitative features and the person’s home ownership status performed best out of the qualitative features.\n\nmax_cv_score\n\n0.848212523277911\n\n\nThe maximum cross validation score produced (which used the features: person_home_ownership, person_emp_length, and loan_percent_income) a score of 0.85 accuracy.\n\nLR.fit(X_train[max_cols], y_train)\nw = LR.coef_\nw\n\narray([[-0.14177704,  0.49223552, -1.20834042,  0.85823919, -0.01913409,\n         8.28689689]])\n\n\nFitting a logistic regression with the max columns and found the weight vector of the model."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-d-find-a-threshold",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-d-find-a-threshold",
    "title": "Optimal Decision Making",
    "section": "Part D: Find a Threshold",
    "text": "Part D: Find a Threshold\n\nfrom sklearn.metrics import confusion_matrix\n\ny_train_pred = LR.predict(X_train[max_cols])\nC = confusion_matrix(y_train,y_train_pred, normalize = \"true\")\nC\n\narray([[0.98253712, 0.01746288],\n       [0.64250914, 0.35749086]])\n\n\nShowing a confusion matrix for the logistic regression model.\n\ndef linear_score(X, w):\n    return X@w\n\ns = linear_score(X_train[max_cols], w[0])\n\nfor t in np.linspace(1,6,11):\n    y_pred = s &gt;= t\n    acc = (y_pred == y_train).mean()\n    print(f\"A threshold of {t:.1f} gives an accuracy of {acc:.2f}.\")\n\nA threshold of 1.0 gives an accuracy of 0.47.\nA threshold of 1.5 gives an accuracy of 0.60.\nA threshold of 2.0 gives an accuracy of 0.72.\nA threshold of 2.5 gives an accuracy of 0.80.\nA threshold of 3.0 gives an accuracy of 0.84.\nA threshold of 3.5 gives an accuracy of 0.84.\nA threshold of 4.0 gives an accuracy of 0.81.\nA threshold of 4.5 gives an accuracy of 0.80.\nA threshold of 5.0 gives an accuracy of 0.79.\nA threshold of 5.5 gives an accuracy of 0.79.\nA threshold of 6.0 gives an accuracy of 0.79.\n\n\nFinding a starting threshold that results in the highest accuracy. The threshold of 3.0 results in the highest accuracy.\n\nt = 3.0\n\navg_loan_amount = X_train[\"loan_amnt\"].mean()\navg_loan_int_rate = X_train[\"loan_int_rate\"].mean()/100\ncost_of_FN = avg_loan_amount*(1 + 0.25*avg_loan_int_rate)**3 - 1.7*avg_loan_amount\ngain_of_TN = avg_loan_amount*(1 + 0.25*avg_loan_int_rate)**10 - avg_loan_amount\n\n# compute the scores\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\n\n# compute error rates\nFPR   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\nTPR   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\n# compute the expected gain\ngain = gain_of_TN*TNR  + cost_of_FN*FNR \ngain\n\n-698.0432952112778\n\n\nCalculating the gain based on profit maximization by calculating the cost of a false negative and the gain of a true negative. The gain on a true negative assumes that the profit earned by the bank on a 10-year loan is equal to 25% of the interest rate each year, with the other 75% of the interest going to things like salaries for the people who manage the bank. The cost of a false negative corresponds to the same profit-earning mechanism as above, but assumes that the borrower defaults three years into the loan and that the bank loses 70% of the principal. Using these formulas, and the threshold that maximized accuracy (3.0), the gain from the model is -698 dollars for each loan.\n\nnum_thresholds = 101\n\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\nT = np.linspace(s.min()-0.1, s.max()+0.1, num_thresholds)\ns = linear_score(X_train[max_cols], w[0])\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds    = s &gt;= t\n    FPR[i]   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i]   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\nIn order to find a threshold that maximizes gain, 101 thresholds are tried from the minimum value of the logistic regression to the maximum value. These different thresholds are stored in arrays recording their false positive and true positive rates.\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\navg_loan_amount = X_train[\"loan_amnt\"].mean()\navg_loan_int_rate = X_train[\"loan_int_rate\"].mean()/100\n\ncost_of_FN = avg_loan_amount*(1 + 0.25*avg_loan_int_rate)**3 - 1.7*avg_loan_amount\ngain_of_TN = avg_loan_amount*(1 + 0.25*avg_loan_int_rate)**10 - avg_loan_amount\n\ngain =  gain_of_TN*TNR  + cost_of_FN*FNR \n\nplt.plot(T, gain)\nplt.gca().set(ylim = (-300, 400), xlim = (-1.5, 3))\nlabs = plt.gca().set(xlabel = r\"Threshold $t$\", ylabel = \"Expected profit per loan\")\nplt.show()\n\n\n\n\n\n\n\n\n332.6396917244905\n\n\nThe false positive and true negative rates are used to calculate false negative and true negative values. The expected profit per loan is then calculated for each threshold and visualized on the line chart.\n\nymax = gain.max()\nxpos = np.where(gain == ymax)\nxmax = T[xpos]\nxmax\n\n/var/folders/nr/wxklmm8n7v1986xxzzsjr73m0000gn/T/ipykernel_94626/2988660966.py:2: DeprecationWarning: Calling nonzero on 0d arrays is deprecated, as it behaves surprisingly. Use `atleast_1d(cond).nonzero()` if the old behavior was intended. If the context of this warning is of the form `arr[nonzero(cond)]`, just use `arr[cond]`.\n  xpos = np.where(gain == ymax)\n\n\n(array([-1.76658]), 389.3279923336545)\n\n\nFinding the threshold with the maximum value using the .where function to find the index and then using that index to find the value in the threshold array. The threshold with the highest expected profit per loan is 1.086.\n\nt = 1.086\n\n# compute the scores\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\n\n# compute error rates\nFPR   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\nTPR   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\n# compute the expected gain\ngain = gain_of_TN*TNR  + cost_of_FN*FNR \ngain\n\n329.09965389194906\n\n\nThe gain for the threshold 1.086 results in a profit of 329 dollars expected profit per loan. This gain is significantly better than the 698 dollars estimated lost per loan when using a threshold that maximizes accuracy."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-e-evaluate-your-model-from-the-banks-perspective",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-e-evaluate-your-model-from-the-banks-perspective",
    "title": "Optimal Decision Making",
    "section": "Part E: Evaluate Your Model from the Bank’s Perspective",
    "text": "Part E: Evaluate Your Model from the Bank’s Perspective\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nX_test, y_test = prepare_data(df_test)\n\nLoading and preparing the test data.\n\nt = 1.086\n\n# compute the scores\ns     = linear_score(X_test[max_cols], w[0])\npreds = s &gt;= t\n\n# compute error rates\nFPR   = ((preds == 1) & (y_test == 0)).sum() / (y_test == 0).sum()\nTPR   = ((preds == 1) & (y_test == 1)).sum() / (y_test == 1).sum()\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\n# compute the expected gain\ngain = gain_of_TN*TNR  + cost_of_FN*FNR \ngain\n\n389.3279923336545\n\n\nThe model with a threshold of 1.086 resulted in a 389 dollar expected gain per loan in the testing data. This profit is greater than the profit produced by the training set!"
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-f-evaluate-your-model-from-the-borrowers-perspective",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-f-evaluate-your-model-from-the-borrowers-perspective",
    "title": "Optimal Decision Making",
    "section": "Part F: Evaluate Your Model From the Borrower’s Perspective",
    "text": "Part F: Evaluate Your Model From the Borrower’s Perspective\nIs it more difficult for people in certain age groups to access credit under your proposed system?\n\ndf4 = df_train[df_train['person_age'] &lt; 40]\nX_train, y_train = prepare_data(df4)\n\n\n# compute the scores\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\npreds.mean()\n\n0.6650836829729356\n\n\n\ndf5 = df_train[df_train['person_age'] &gt; 40]\nX_train, y_train = prepare_data(df5)\n\n# compute the scores\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\npreds.mean()\n\n0.6085271317829457\n\n\nAlthough I didn’t use age as a predictor in my logistic regression model, I found that in the training data, people under the age of 40 were able to receive a loan at a rate of 66%; whereas, people over the age of 40 were only able to receive a loan at a rate of 60%. This difference could be due to other predictors in my model, such as the person’s employment length or home ownership status.\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\n\ndf6 = df_train[df_train['loan_intent'] == \"MEDICAL\"]\ndefault_rate = df6[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df6)\n\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.7011848341232227\nPercentage defaulted = 0.26328852119958635\n\n\n\ndf7 = df_train[df_train['loan_intent'] == \"EDUCATION\"]\ndefault_rate = df7[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df7)\n\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.6729240282685512\nPercentage defaulted = 0.17339574800078017\n\n\n\ndf8 = df_train[df_train['loan_intent'] == \"VENTURE\"]\ndefault_rate = df8[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df8)\n\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.6247213277186029\nPercentage defaulted = 0.14867793671434765\n\n\n\ndf9 = df_train[df_train['loan_intent'] == \"HOMEIMPROVEMENT\"]\ndefault_rate = df9[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df9)\n\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.6076684740511231\nPercentage defaulted = 0.26464507236388696\n\n\n\ndf10 = df_train[df_train['loan_intent'] == \"PERSONAL\"]\ndefault_rate = df10[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df10)\n\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.6478473833462233\nPercentage defaulted = 0.19373865698729584\n\n\n\ndf11 = df_train[df_train['loan_intent'] == \"DEBTCONSOLIDATION\"]\ndefault_rate = df11[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df11)\n\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.6970773012838022\nPercentage defaulted = 0.2874581139301101\n\n\nBased on the training data, the intent category least likely to receive a loan was home improvement with 61% received. The intent category most likely to receive a loan was medical with 70% received. Interestingly, the venture intent category received only 62% of requests, but only had a default rate of 15%.\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\nmedian = df_train[\"person_income\"].median()\ndf12 = df_train[df_train['person_income'] &lt; median]\ndefault_rate = df12[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df12)\n\n\n# compute the scores\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.7921593596507186\nPercentage defaulted = 0.31125462707726237\n\n\n\nmedian = df_train[\"person_income\"].median()\ndf13 = df_train[df_train['person_income'] &gt; median]\ndefault_rate = df13[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df13)\n\n\n# compute the scores\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.5379828882551205\nPercentage defaulted = 0.1302071302071302\n\n\nUnder my decision system, requestors with an income level above the median received loans at a rate of 53 percent. Compared to requestors with an income level below the median, who received loans at a rate of 79 percent."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-g-write-and-reflect",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-g-write-and-reflect",
    "title": "Optimal Decision Making",
    "section": "Part G: Write and Reflect",
    "text": "Part G: Write and Reflect\nThrough the use of an exhaustive feature search and cross validation, I created a model that uses employment length, the percentage that the loan is compared to the borrower’s income, and the home ownership status to predict if the loan will default with 86% accuracy in the training data. Further, I found a threshold value of 1.086 that maximized the banks profit with an average gain of 333 in the training data and 389 in the testing data.\nAlthough I’ve worked with pandas dataframes and creating seaborn visualizations, this process was helpful practice for me. More importantly, this was my first implementation of a threshold to maximize gain. This post was helpful to my understanding of the concept as a whole and brought me through all the implementation steps.\nThe concept of fairness is important to discuss when evaluating a model that decides who does and who doesn’t receive loans. To reiterate, this model makes decisions purely to maximize gain without accounting for any other factors. The model made no attempt to adjust for social inequalities or other factors that would make it more just. However, in terms of fairness, which I classify as impartial decision making, given the intentions of the model, I think it performs in a fair manner. For example, considering people who are seeking loans for medical expenses have high rates of default, with intentions of pure gain, I think it is fair for a model to be less likely to give these people loans. This does not mean that it is morally right to not give these people loans, but considering their higher default rates, it is fair."
  }
]