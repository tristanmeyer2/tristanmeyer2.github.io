[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog Test"
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this post was to create a model that predicts a penguin’s species based on both quantitative and qualitative data. Personally, this post demonstrates my ability to work with and manipulate dataframes in Pandas, create visualizations using seaborn, choose effective features, and build an effective model. My model uses Culmen Depth, Culmen Length and the island of inhabitants of the penguins in the Palmer Penguin data set to predict the species of the penguin. I used a brute force tactic to choose these three features: iterating through every combination of features and fitting a logistic regression to each combination. I compared the mean cross validation scores for each combination and chose the three features with the highest score. I displayed the results of my model on both the test and training data, showing the prediction areas produced by the features."
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#abstract",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this post was to create a model that predicts a penguin’s species based on both quantitative and qualitative data. Personally, this post demonstrates my ability to work with and manipulate dataframes in Pandas, create visualizations using seaborn, choose effective features, and build an effective model. My model uses Culmen Depth, Culmen Length and the island of inhabitants of the penguins in the Palmer Penguin data set to predict the species of the penguin. I used a brute force tactic to choose these three features: iterating through every combination of features and fitting a logistic regression to each combination. I compared the mean cross validation scores for each combination and chose the three features with the highest score. I displayed the results of my model on both the test and training data, showing the prediction areas produced by the features."
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#explore",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#explore",
    "title": "Classifying Palmer Penguins",
    "section": "Explore:",
    "text": "Explore:\nPlease create two interesting visualizations of the data, and one summary table (e.g. compute the average or median value of some features, by group). Your visualizations and table should:\n\nInclude axis labels and legends.\nHelp you draw conclusions about what features you are going to try using for your model.\nBe accompanied by discussion of what you learned and how you will use it in modeling. Most figures and tables are worth at least one short paragraph of discussion.\n\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nImporting the Palmer Penguin Dataset.\n\nimport seaborn as sns\n\ndf1 = train[[\"Sex\",\"Body Mass (g)\", \"Species\"]]\ndf1 = df1.drop(df1[df1[\"Sex\"] == \".\"].index)\ndf1[\"Species\"] = df1[\"Species\"].str.split().str.get(0)\ndf1 = df1.dropna()\n\np1 = sns.boxplot(df1, x = \"Sex\", y = \"Body Mass (g)\", hue = \"Species\").set_title(\"Comparing Penguins' Body Mass and Sex across Species\")\n\n\n\n\n\n\n\n\nI created a new data frame with the the Sex, Body Mass, and Species columns. I removed the penguins with the third sex (not male or female) because there were not enough specimens with this sex to make an impact on this visualization. Further, I dropped all of the rows with NA values for any of these features. I then created a boxplot with Sex on the X-axis and Body Mass on the Y-axis with the individual boxplots separated by species. This plot shows that the average body mass of all species is greater for male penguins compared to females. Further, (and important for feature selection) it shows the similarities between body mass between the Chinstrap and Adelie species.\n\ndf2 = train[[\"Body Mass (g)\", \"Flipper Length (mm)\"]]\ndf2 = df2.dropna()\n\np2 = sns.scatterplot(df2, x = \"Body Mass (g)\", y = \"Flipper Length (mm)\", color = \"blue\").set_title(\"Comparing Flipper Length and Body Mass of Penguins\")\n\n\n\n\n\n\n\n\nI created another data frame (dropping the NA values again) including the body mass and flipper length features. I displayed these features using a scatter plot. This visualization shows that there is a positive correlation between body mass and flipper length among all the penguins in the data.\n\ndf3 = train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island\"]]\ndf3 = df3.dropna()\n\np3 = sns.scatterplot(df3, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue=\"Island\").set_title(\"Comparing Culmen Length and Culmen Depth across Islands\")\n\n\n\n\n\n\n\n\nFor my final visualization, I created another data frame that included the culmen length, culmen depth, and island features. This visualization shows that particular ranges of culmen length and depth could be associated with different islands. For example, generally, penguins with culmen depth under 16 mm and culmen length over 43 mm belong to the Biscoe island.\n\ndf4 = train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Sex\"]]\ndf4.groupby(\"Sex\").aggregate('mean')\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\nSex\n\n\n\n\n\n\n\n\n.\n44.500000\n15.700000\n217.000000\n4875.000000\n\n\nFEMALE\n42.551128\n16.371429\n198.067669\n3882.330827\n\n\nMALE\n45.463359\n17.993130\n203.687023\n4521.755725\n\n\n\n\n\n\n\nI created this summary table by grouping the penguins and sex and aggregating the culmen length, culmen depth, flipper length, and body mass features by their mean values. Through this table, the differences in these quantitative values between sexes."
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#model",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#model",
    "title": "Classifying Palmer Penguins",
    "section": "Model:",
    "text": "Model:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nPreparing the qualitative columns in the dataset and removing the Species column. I also removed the other identifying columns for the penguins and removed the third sex (not female or male) because there are a small number of penguins with this sex. I also one hot encoded the categorical feature columns.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Stage_Adult, 1 Egg Stage\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nmax_cv_score = 0\nmax_cols = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression(max_iter=10000)\n    LR.fit(X_train[cols], y_train)\n    cv_scores = cross_val_score(LR,X_train[cols],y_train,cv = 10)\n    cv_score_mean = cv_scores.mean()\n    if cv_score_mean &gt; max_cv_score:\n      max_cv_score = cv_score_mean\n      max_cols = cols\nmax_cols\n\n['Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen',\n 'Culmen Length (mm)',\n 'Culmen Depth (mm)']\n\n\nI split the features into the quantitative columns and the qualitative columns. Next, for every qualitative feature, I iterated through every combination of two quantitative features. For each of these combinations of one qualitative and two quantitative features, I fit a logistic regression model and calculated the mean cross evaluation score over ten folds. I kept track of the greatest mean cross evaluation score and found that Culmen Length, Culmen Depth, and Island performed best.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\nmax_cols.reverse()\n\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[max_cols], y_train)\nLR.score(X_test[max_cols], y_test)\n\n1.0\n\n\nI imported the test data, and fit a logistic regression on the training data with the columns that performed the best in the training data. I then scored the model using the testing data and found that it performed to 100% accuracy.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[max_cols], y_train)\n\n\n\n\n\n\n\n\nThis plot shows the prediction regions for my logistic regression model. As shown in the plot, the model uses culmen length and depth and island of inhabitants to make it’s predictions. The training data is shown in this visualization\n\nplot_regions(LR, X_test[max_cols], y_test)\n\n\n\n\n\n\n\n\nThe plot above uses the test data instead of the training data over the same variables. As shown in the plot, the model performed with one hundred percent accuracy for the test data.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[max_cols])\nC = confusion_matrix(y_test,y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nThe above confusion matrix shows that all of the Adelie penguins were classified as Adelie penguins, all of the Chinstrap penguins were classified as Chinstrap penguins, and all of the Gentoo penguins were classified as Gentoo."
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#discussion",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion:",
    "text": "Discussion:\nThrough the use of an exhaustive feature search and cross validation, I created a model that uses culmen depth, culmen length, and the island of inhabitants to predict the species of the penguin. My model performed with 99.6 percent accuracy in the training data with a cross validation score of 99.6 percent as well, and performed with one hundred percent accuracy in the test data.\nThrough this process I improved my skills with pandas data frames and fitting linear regression models. I also learned about feature selection. I experimented with multiple tools in the scikit learn database (VarianceThreshold, SelectKBest, etc.), but I didn’t end up using any of these tools for my selection. I learned how to test every combination of features and practiced my skills with cross validation on each of these combinations."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html",
    "title": "Optimal Decision Making",
    "section": "",
    "text": "The aim of this post was to create a model to maximize the gain on loans for a bank. This post demonstrates my ability to create visualizations in seaborn, choose featuers for my model, threshold my model, and analyze my model from the point of view of both the loaner and the borrower. My model uses the person’s home ownership status, the percent of their income that the loan is, and the length of their most recent employment to predict whether or not the borrower defaulted on the loan. My model performs at 85 percent accuracy in the test data. I used a brute force tactic to choose these three features: iterating through every combination of features and fitting a logistic regression to each combination. I compared the mean cross validation scores for each combination and chose the three features with the highest score. I found the threshold that maximized gain for the bank by testing a wide range of values. I calculated the gain based on profit maximization by calculating the cost of a false negative and the gain of a true negative. I found that a threshold value of 1.086 maximized the banks profit with an average gain of 333 in the training data and 389 in the testing data."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#abstract",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#abstract",
    "title": "Optimal Decision Making",
    "section": "",
    "text": "The aim of this post was to create a model to maximize the gain on loans for a bank. This post demonstrates my ability to create visualizations in seaborn, choose featuers for my model, threshold my model, and analyze my model from the point of view of both the loaner and the borrower. My model uses the person’s home ownership status, the percent of their income that the loan is, and the length of their most recent employment to predict whether or not the borrower defaulted on the loan. My model performs at 85 percent accuracy in the test data. I used a brute force tactic to choose these three features: iterating through every combination of features and fitting a logistic regression to each combination. I compared the mean cross validation scores for each combination and chose the three features with the highest score. I found the threshold that maximized gain for the bank by testing a wide range of values. I calculated the gain based on profit maximization by calculating the cost of a false negative and the gain of a true negative. I found that a threshold value of 1.086 maximized the banks profit with an average gain of 333 in the training data and 389 in the testing data."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-a-grab-the-data",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-a-grab-the-data",
    "title": "Optimal Decision Making",
    "section": "Part A: Grab the Data",
    "text": "Part A: Grab the Data\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\nMORTGAGE\n8.0\nEDUCATION\nA\n3000\n7.29\n0\n0.02\nN\n17\n\n\n26060\n23\n48000\nRENT\n1.0\nVENTURE\nA\n4325\n5.42\n0\n0.09\nN\n4\n\n\n26061\n22\n60000\nRENT\n0.0\nMEDICAL\nB\n15000\n11.71\n0\n0.25\nN\n4\n\n\n26062\n30\n144000\nMORTGAGE\n12.0\nPERSONAL\nC\n35000\n12.68\n0\n0.24\nN\n8\n\n\n26063\n25\n60000\nRENT\n5.0\nEDUCATION\nA\n21450\n7.29\n1\n0.36\nN\n4\n\n\n\n\n26064 rows × 12 columns\n\n\n\nDownloading the training data"
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-b-exploring-the-data",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-b-exploring-the-data",
    "title": "Optimal Decision Making",
    "section": "Part B: Exploring the Data",
    "text": "Part B: Exploring the Data\n\nimport seaborn as sns\n\ndf1 = df_train[[\"loan_intent\", \"loan_amnt\"]]\ndf1 = df1.dropna()\n\np1 = sns.histplot(data=df1, x=\"loan_amnt\", hue=\"loan_intent\", multiple=\"stack\", bins = 10)\np1.set(xlabel=\"Loan Amount\", title=\"Visualizing Loan Intent over Differing Loan Amounts\")\np1\n\n/Users/tristanmeyer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThis visualization shows a histogram of loan amounts shown over twenty bins separated by loan intent. Shown by the visualization, the majority of loans occur in the $5,000 - $15,000 range. Further, over the range of loan amounts, the proportions of the different loan intents stays relatively steady.\n\ndf2 = df_train[[\"person_age\", \"loan_int_rate\"]]\ndf2 = df2.dropna()\ndf2['cat_age'] = np.select([df2.person_age &lt; 30, df2.person_age &lt; 40, df2.person_age &lt; 50, df2.person_age &lt; 60, df2.person_age &lt; 70,], ['&lt;30', '30-40','40-50','50-60','60-70'], '&gt;70')\ndf2 = df2.groupby(\"cat_age\", sort=False).aggregate('mean')\n\np2 = sns.lineplot(data=df2, x=\"cat_age\", y = \"loan_int_rate\")\np2.set(xlabel =\"Ages\", ylabel = \"Loan Interest Rates (%)\", title =\"Visualization Loan Interest Rates over Different Ages\")\np2\n\n/Users/tristanmeyer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/tristanmeyer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nTo create this visualization, I created a new column that grouped the ages into ten year spans. Then, I aggregated the data frame by the mean values for the different categories in this new column. I then visualized this data over a line plot with the age categories on the x axis and the loan interest rates on the y axis. The graph shows a fluctuation between 11 to 11.1 for the ages up to 70 years old and then a jump to 11.3 for the loans given to people over 70.\n\ndf3 = df_train[[\"person_age\", \"person_income\", \"person_home_ownership\", \"loan_amnt\"]]\ndf3 = df3.dropna()\ndf3.groupby(\"person_home_ownership\", sort=False).aggregate('mean')\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nloan_amnt\n\n\nperson_home_ownership\n\n\n\n\n\n\n\nRENT\n27.532802\n54961.066515\n8843.507973\n\n\nMORTGAGE\n28.005129\n81076.729087\n10562.137462\n\n\nOWN\n27.639462\n57348.641383\n8978.912626\n\n\nOTHER\n27.159091\n78263.238636\n11235.795455\n\n\n\n\n\n\n\nThis summary table shows the difference between the loan receivers age, income, and loan amount over the different home ownership statuses. The table shows that age remains relatively constant for all categories of home ownership. The table also shows an increased income for those with ‘mortgage’ and ‘other’ statuses compared to the ‘rent’ and ‘own’ statuses. The loan amount follows a similar pattern to the income, the ‘mortgage’ and ‘other’ statuses are greater than the ‘rent’ and ‘own’ statuses."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-c-build-a-model",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-c-build-a-model",
    "title": "Optimal Decision Making",
    "section": "Part C: Build a Model",
    "text": "Part C: Build a Model\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndef prepare_data(df):\n  df = df.drop([\"loan_grade\"], axis = 1)\n  df = df.dropna()\n  y = df[\"loan_status\"]\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\nPreparing the qualitative columns in the dataset and removing the loan status and loan grade columns. I also one hot encoded the categorical feature columns.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nall_quant_cols = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\nall_qual_cols = [\"person_home_ownership\",\"loan_intent\",\"cb_person_default_on_file\"]\nmax_cv_score = 0\nmax_cols = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression(max_iter=10000)\n    LR.fit(X_train[cols], y_train)\n    cv_scores = cross_val_score(LR,X_train[cols],y_train,cv = 10)\n    cv_score_mean = cv_scores.mean()\n    if cv_score_mean &gt; max_cv_score:\n      max_cv_score = cv_score_mean\n      max_cols = cols\nmax_cols\n\n['person_home_ownership_MORTGAGE',\n 'person_home_ownership_OTHER',\n 'person_home_ownership_OWN',\n 'person_home_ownership_RENT',\n 'person_emp_length',\n 'loan_percent_income']\n\n\nI split the features into the quantitative columns and the qualitative columns. Next, for every qualitative feature, I iterated through every combination of every possible number of quantitative features. For each of these combinations of one qualitative and different number of quantitative features, I fit a logistic regression model and calculated the mean cross evaluation score over ten folds. I found that a combination between two quantitative features performed just as well as combinations of three and four quantitative features and combinations of five and six quantitative features performed worse. Therefore, with two quantitative features and one qualitative feature the person’s length of employment and the percent income that the loan is performed best as quantitative features and the person’s home ownership status performed best out of the qualitative features.\n\nmax_cv_score\n\n0.848212523277911\n\n\nThe maximum cross validation score produced (which used the features: person_home_ownership, person_emp_length, and loan_percent_income) a score of 0.85 accuracy.\n\nLR.fit(X_train[max_cols], y_train)\nw = LR.coef_\nw\n\narray([[-0.14177704,  0.49223552, -1.20834042,  0.85823919, -0.01913409,\n         8.28689689]])\n\n\nFitting a logistic regression with the max columns and found the weight vector of the model."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-d-find-a-threshold",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-d-find-a-threshold",
    "title": "Optimal Decision Making",
    "section": "Part D: Find a Threshold",
    "text": "Part D: Find a Threshold\n\nfrom sklearn.metrics import confusion_matrix\n\ny_train_pred = LR.predict(X_train[max_cols])\nC = confusion_matrix(y_train,y_train_pred, normalize = \"true\")\nC\n\narray([[0.98253712, 0.01746288],\n       [0.64250914, 0.35749086]])\n\n\nShowing a confusion matrix for the logistic regression model.\n\ndef linear_score(X, w):\n    return X@w\n\ns = linear_score(X_train[max_cols], w[0])\n\nfor t in np.linspace(1,6,11):\n    y_pred = s &gt;= t\n    acc = (y_pred == y_train).mean()\n    print(f\"A threshold of {t:.1f} gives an accuracy of {acc:.2f}.\")\n\nA threshold of 1.0 gives an accuracy of 0.47.\nA threshold of 1.5 gives an accuracy of 0.60.\nA threshold of 2.0 gives an accuracy of 0.72.\nA threshold of 2.5 gives an accuracy of 0.80.\nA threshold of 3.0 gives an accuracy of 0.84.\nA threshold of 3.5 gives an accuracy of 0.84.\nA threshold of 4.0 gives an accuracy of 0.81.\nA threshold of 4.5 gives an accuracy of 0.80.\nA threshold of 5.0 gives an accuracy of 0.79.\nA threshold of 5.5 gives an accuracy of 0.79.\nA threshold of 6.0 gives an accuracy of 0.79.\n\n\nFinding a starting threshold that results in the highest accuracy. The threshold of 3.0 results in the highest accuracy.\n\nt = 3.0\n\navg_loan_amount = X_train[\"loan_amnt\"].mean()\navg_loan_int_rate = X_train[\"loan_int_rate\"].mean()/100\ncost_of_FN = avg_loan_amount*(1 + 0.25*avg_loan_int_rate)**3 - 1.7*avg_loan_amount\ngain_of_TN = avg_loan_amount*(1 + 0.25*avg_loan_int_rate)**10 - avg_loan_amount\n\n# compute the scores\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\n\n# compute error rates\nFPR   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\nTPR   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\n# compute the expected gain\ngain = gain_of_TN*TNR  + cost_of_FN*FNR \ngain\n\n-698.0432952112778\n\n\nCalculating the gain based on profit maximization by calculating the cost of a false negative and the gain of a true negative. The gain on a true negative assumes that the profit earned by the bank on a 10-year loan is equal to 25% of the interest rate each year, with the other 75% of the interest going to things like salaries for the people who manage the bank. The cost of a false negative corresponds to the same profit-earning mechanism as above, but assumes that the borrower defaults three years into the loan and that the bank loses 70% of the principal. Using these formulas, and the threshold that maximized accuracy (3.0), the gain from the model is -698 dollars for each loan.\n\nnum_thresholds = 101\n\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\nT = np.linspace(s.min()-0.1, s.max()+0.1, num_thresholds)\ns = linear_score(X_train[max_cols], w[0])\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds    = s &gt;= t\n    FPR[i]   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i]   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\nIn order to find a threshold that maximizes gain, 101 thresholds are tried from the minimum value of the logistic regression to the maximum value. These different thresholds are stored in arrays recording their false positive and true positive rates.\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\navg_loan_amount = X_train[\"loan_amnt\"].mean()\navg_loan_int_rate = X_train[\"loan_int_rate\"].mean()/100\n\ncost_of_FN = avg_loan_amount*(1 + 0.25*avg_loan_int_rate)**3 - 1.7*avg_loan_amount\ngain_of_TN = avg_loan_amount*(1 + 0.25*avg_loan_int_rate)**10 - avg_loan_amount\n\ngain =  gain_of_TN*TNR  + cost_of_FN*FNR \n\nplt.plot(T, gain)\nplt.gca().set(ylim = (-300, 400), xlim = (-1.5, 3))\nlabs = plt.gca().set(xlabel = r\"Threshold $t$\", ylabel = \"Expected profit per loan\")\nplt.show()\n\n\n\n\n\n\n\n\n332.6396917244905\n\n\nThe false positive and true negative rates are used to calculate false negative and true negative values. The expected profit per loan is then calculated for each threshold and visualized on the line chart.\n\nymax = gain.max()\nxpos = np.where(gain == ymax)\nxmax = T[xpos]\nxmax\n\n/var/folders/nr/wxklmm8n7v1986xxzzsjr73m0000gn/T/ipykernel_94626/2988660966.py:2: DeprecationWarning: Calling nonzero on 0d arrays is deprecated, as it behaves surprisingly. Use `atleast_1d(cond).nonzero()` if the old behavior was intended. If the context of this warning is of the form `arr[nonzero(cond)]`, just use `arr[cond]`.\n  xpos = np.where(gain == ymax)\n\n\n(array([-1.76658]), 389.3279923336545)\n\n\nFinding the threshold with the maximum value using the .where function to find the index and then using that index to find the value in the threshold array. The threshold with the highest expected profit per loan is 1.086.\n\nt = 1.086\n\n# compute the scores\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\n\n# compute error rates\nFPR   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\nTPR   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\n# compute the expected gain\ngain = gain_of_TN*TNR  + cost_of_FN*FNR \ngain\n\n329.09965389194906\n\n\nThe gain for the threshold 1.086 results in a profit of 329 dollars expected profit per loan. This gain is significantly better than the 698 dollars estimated lost per loan when using a threshold that maximizes accuracy."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-e-evaluate-your-model-from-the-banks-perspective",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-e-evaluate-your-model-from-the-banks-perspective",
    "title": "Optimal Decision Making",
    "section": "Part E: Evaluate Your Model from the Bank’s Perspective",
    "text": "Part E: Evaluate Your Model from the Bank’s Perspective\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nX_test, y_test = prepare_data(df_test)\n\nLoading and preparing the test data.\n\nt = 1.086\n\n# compute the scores\ns     = linear_score(X_test[max_cols], w[0])\npreds = s &gt;= t\n\n# compute error rates\nFPR   = ((preds == 1) & (y_test == 0)).sum() / (y_test == 0).sum()\nTPR   = ((preds == 1) & (y_test == 1)).sum() / (y_test == 1).sum()\n\nTNR = 1 - FPR\nFNR = 1 - TPR\n\n# compute the expected gain\ngain = gain_of_TN*TNR  + cost_of_FN*FNR \ngain\n\n389.3279923336545\n\n\nThe model with a threshold of 1.086 resulted in a 389 dollar expected gain per loan in the testing data. This profit is greater than the profit produced by the training set!"
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-f-evaluate-your-model-from-the-borrowers-perspective",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-f-evaluate-your-model-from-the-borrowers-perspective",
    "title": "Optimal Decision Making",
    "section": "Part F: Evaluate Your Model From the Borrower’s Perspective",
    "text": "Part F: Evaluate Your Model From the Borrower’s Perspective\nIs it more difficult for people in certain age groups to access credit under your proposed system?\n\ndf4 = df_train[df_train['person_age'] &lt; 40]\nX_train, y_train = prepare_data(df4)\n\n\n# compute the scores\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\npreds.mean()\n\n0.6650836829729356\n\n\n\ndf5 = df_train[df_train['person_age'] &gt; 40]\nX_train, y_train = prepare_data(df5)\n\n# compute the scores\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\npreds.mean()\n\n0.6085271317829457\n\n\nAlthough I didn’t use age as a predictor in my logistic regression model, I found that in the training data, people under the age of 40 were able to receive a loan at a rate of 66%; whereas, people over the age of 40 were only able to receive a loan at a rate of 60%. This difference could be due to other predictors in my model, such as the person’s employment length or home ownership status.\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\n\ndf6 = df_train[df_train['loan_intent'] == \"MEDICAL\"]\ndefault_rate = df6[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df6)\n\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.7011848341232227\nPercentage defaulted = 0.26328852119958635\n\n\n\ndf7 = df_train[df_train['loan_intent'] == \"EDUCATION\"]\ndefault_rate = df7[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df7)\n\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.6729240282685512\nPercentage defaulted = 0.17339574800078017\n\n\n\ndf8 = df_train[df_train['loan_intent'] == \"VENTURE\"]\ndefault_rate = df8[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df8)\n\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.6247213277186029\nPercentage defaulted = 0.14867793671434765\n\n\n\ndf9 = df_train[df_train['loan_intent'] == \"HOMEIMPROVEMENT\"]\ndefault_rate = df9[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df9)\n\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.6076684740511231\nPercentage defaulted = 0.26464507236388696\n\n\n\ndf10 = df_train[df_train['loan_intent'] == \"PERSONAL\"]\ndefault_rate = df10[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df10)\n\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.6478473833462233\nPercentage defaulted = 0.19373865698729584\n\n\n\ndf11 = df_train[df_train['loan_intent'] == \"DEBTCONSOLIDATION\"]\ndefault_rate = df11[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df11)\n\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.6970773012838022\nPercentage defaulted = 0.2874581139301101\n\n\nBased on the training data, the intent category least likely to receive a loan was home improvement with 61% received. The intent category most likely to receive a loan was medical with 70% received. Interestingly, the venture intent category received only 62% of requests, but only had a default rate of 15%.\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\nmedian = df_train[\"person_income\"].median()\ndf12 = df_train[df_train['person_income'] &lt; median]\ndefault_rate = df12[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df12)\n\n\n# compute the scores\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.7921593596507186\nPercentage defaulted = 0.31125462707726237\n\n\n\nmedian = df_train[\"person_income\"].median()\ndf13 = df_train[df_train['person_income'] &gt; median]\ndefault_rate = df13[\"loan_status\"].mean()\nX_train, y_train = prepare_data(df13)\n\n\n# compute the scores\ns     = linear_score(X_train[max_cols], w[0])\npreds = s &gt;= t\nprint(\"Percentage given loans = \" + str(preds.mean()))\nprint(\"Percentage defaulted = \" + str(default_rate))\n\nPercentage given loans = 0.5379828882551205\nPercentage defaulted = 0.1302071302071302\n\n\nUnder my decision system, requestors with an income level above the median received loans at a rate of 53 percent. Compared to requestors with an income level below the median, who received loans at a rate of 79 percent."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-g-write-and-reflect",
    "href": "posts/OptimalDecisionMaking/OptimalDecisionMaking.html#part-g-write-and-reflect",
    "title": "Optimal Decision Making",
    "section": "Part G: Write and Reflect",
    "text": "Part G: Write and Reflect\nThrough the use of an exhaustive feature search and cross validation, I created a model that uses employment length, the percentage that the loan is compared to the borrower’s income, and the home ownership status to predict if the loan will default with 86% accuracy in the training data. Further, I found a threshold value of 1.086 that maximized the banks profit with an average gain of 333 in the training data and 389 in the testing data.\nAlthough I’ve worked with pandas dataframes and creating seaborn visualizations, this process was helpful practice for me. More importantly, this was my first implementation of a threshold to maximize gain. This post was helpful to my understanding of the concept as a whole and brought me through all the implementation steps.\nThe concept of fairness is important to discuss when evaluating a model that decides who does and who doesn’t receive loans. To reiterate, this model makes decisions purely to maximize gain without accounting for any other factors. The model made no attempt to adjust for social inequalities or other factors that would make it more just. However, in terms of fairness, which I classify as impartial decision making, given the intentions of the model, I think it performs in a fair manner. For example, considering people who are seeking loans for medical expenses have high rates of default, with intentions of pure gain, I think it is fair for a model to be less likely to give these people loans. This does not mean that it is morally right to not give these people loans, but considering their higher default rates, it is fair."
  },
  {
    "objectID": "posts/LogisticRegression/LogisticRegression.html",
    "href": "posts/LogisticRegression/LogisticRegression.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/LogisticRegression/LogisticRegression.html#abstract",
    "href": "posts/LogisticRegression/LogisticRegression.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nThis blog post explores logistic regression. I used previous code to create a linear model, then implemented logistic regression off of the linear model. My logistic regression model includes a logistic regression class and a gradient descent optimizer. I then used these classes to experiment on several data sets with different gradient descent strategies. To start, I conducted experiments around vanilla gradient descent (\\(\\beta\\) = 0), which gradually reduced the loss of the model. I compared vanilla gradient descent to spicy gradient descent (which uses momentum). I found that spicy gradient descent affects the loss far less gradually, and reduces loss more effectively. Finally, I experimented with overfitting the model to a training data set with more dimensions that points. I created a model that performed to 100% accuracy in the training data, but only around 80% in the testing data, demonstrating overfitting."
  },
  {
    "objectID": "posts/LogisticRegression/LogisticRegression.html#part-a-implement-logistic-regression",
    "href": "posts/LogisticRegression/LogisticRegression.html#part-a-implement-logistic-regression",
    "title": "Implementing Logistic Regression",
    "section": "Part A: Implement Logistic Regression",
    "text": "Part A: Implement Logistic Regression\nSource code of my implementation of logistic regression: https://github.com/tristanmeyer2/tristanmeyer2.github.io/blob/main/posts/LogisticRegression/logistic.py"
  },
  {
    "objectID": "posts/LogisticRegression/LogisticRegression.html#part-b-experiments",
    "href": "posts/LogisticRegression/LogisticRegression.html#part-b-experiments",
    "title": "Implementing Logistic Regression",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments\n\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix]*2-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nDefining three methods: plotting data, drawing a line on a visualization, creating data. The plot_data() function plots two dimensional data on a scatter plot The draw_line() function draws a “separating” line that maximizes classification accuracy on two dimensional data."
  },
  {
    "objectID": "posts/LogisticRegression/LogisticRegression.html#vanilla-gradient-descent",
    "href": "posts/LogisticRegression/LogisticRegression.html#vanilla-gradient-descent",
    "title": "Implementing Logistic Regression",
    "section": "Vanilla Gradient Descent",
    "text": "Vanilla Gradient Descent\nVanilla gradient descent is characterized by \\(\\beta\\) = 0, and \\(\\alpha\\) being sufficiently small\n\ntorch.manual_seed(1234)\n\nX, y = classification_data(noise = 0.5)\n\nGenerating data points.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_data(X, y, ax)\n\n\n\n\n\n\n\n\nVisualizing the data in a scatter plot. As the plot shows, this data is not linearly separable and has a fair amount of overlap between the two classifications of the data.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss = 1\nloss_vec_van = []\nmaxIterations = 1500\ncurrIteration = 0\n\nwhile loss &gt; 0 and currIteration &lt; maxIterations:\n    loss = LR.loss(X, y) \n    loss_vec_van.append(loss)\n    opt.step(X, y, alpha = 0.2, beta = 0)\n    currIteration += 1\n\nImplementing and running a training loop over a maximum of 100 iterations, calling the step() function in my logistic regression model and keeping track of the loss at each iteration. As mentioned, vanilla gradient descent involveds a \\(\\beta\\) = 0 and in this case an \\(\\alpha\\) = 0.2\n\nplt.plot(loss_vec_van, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_van)), loss_vec_van, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThis visualization shows the progression of the loss over 1500 iterations. As the plot shows, using vanilla gradient descent over 1500 iterations, the models loss converges around 0.20.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_data(X, y, ax)\ndraw_line(LR.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nThis visualization shows the classification data with a plotted line that minimizes the empirical risk of the model."
  },
  {
    "objectID": "posts/LogisticRegression/LogisticRegression.html#benefits-of-momentum",
    "href": "posts/LogisticRegression/LogisticRegression.html#benefits-of-momentum",
    "title": "Implementing Logistic Regression",
    "section": "Benefits of Momentum",
    "text": "Benefits of Momentum\n\ntorch.manual_seed(1235)\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss = 1\nloss_vec = []\nmaxIterations = 100\ncurrIteration = 0\n\n\nwhile loss &gt; 0 and currIteration &lt; maxIterations:\n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n    opt.step(X, y, alpha = 0.2, beta = 0.9)\n    currIteration += 1\n\nTo demonstrate gradient descent with momentum, the same training loop is used as in vanilla gradient descent; however, the \\(\\beta\\) = 0.9.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThis visualization shows the progression of the loss over 100 iterations. As the plot shows, using momentum in gradient descent over 100 iterations, the models loss decreases to around 0.2.\n\nplt.plot(loss_vec_van[:100], color = \"blue\")\nplt.scatter(torch.arange(100), loss_vec_van[:100], color = \"blue\", label=\"Vanilla\")\nplt.legend([\"Vanilla\"])\nplt.plot(loss_vec[:100], color = \"green\")\nplt.scatter(torch.arange(100), loss_vec[:100], color = \"green\", label=\"Spicy\")\nplt.legend()\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThis plot shows the difference between vanilla gradient descent and gradient descent with momentum over the same number of iterations. As shown, although the loss spikes to a value over 1.8, the function reaches a value around 0.2 in only 40 iterations, which is much faster than the vanilla gradient descent reaching a loss value of around 0.3 in 100 iterations."
  },
  {
    "objectID": "posts/LogisticRegression/LogisticRegression.html#overfitting",
    "href": "posts/LogisticRegression/LogisticRegression.html#overfitting",
    "title": "Implementing Logistic Regression",
    "section": "Overfitting",
    "text": "Overfitting\n\ntorch.manual_seed(1)\nX_train, y_train = classification_data(n_points= 50, noise = 0.3, p_dims= 100)\ntorch.manual_seed(2)\nX_test, y_test = classification_data(n_points= 50, noise = 0.3, p_dims= 100)\n\nGenerating two sets of data (training and testing); both sets have 50 points and the 100 dimensions, with a noise value of 0.3.\n\ntorch.manual_seed(1238)\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss = 1\nloss_vec = []\nmaxIterations = 1000\ncurrIteration = 0\n\n\nwhile loss &gt; 0 and currIteration &lt; maxIterations:\n    loss = LR.loss(X_train, y_train) \n    loss_vec.append(loss)\n    opt.step(X_train, y_train, alpha = 0.2, beta = 0.9)\n    currIteration += 1\n\nRunning a training loop on the training dataset over 1000 iterations, with \\(\\beta\\) = 0.9 and \\(\\alpha\\) = 0.2.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThis visualization shows the progression of the loss over 1000 iterations on the training dataset. As the plot shows, over 1000 iterations, the model reaches a value near 0.\n\nfrom sklearn.metrics import accuracy_score\npreds = LR.predict(X_train)\nprint(f'Accuracy: {accuracy_score(y_train, preds)*100:.2f}%')\n\nAccuracy: 100.00%\n\n\nUsing the predict function in our linear model and the accuracy_score function, our model performs to 100.00% accuracy on the training data.\n\npreds_test = LR.predict(X_test)\nprint(f'Accuracy on test data: {accuracy_score(y_test, preds_test)*100:.2f}%')\n\nAccuracy on test data: 82.00%\n\n\nIf our trained model is used to predict the data in the test dataset (using the same functions as above), the accuracy of the model decreases to 82.00% on the testing data. This difference between accuracy in our training data and testing data implies that the model is overfit to the training data, resulting in loss in the testing data."
  },
  {
    "objectID": "posts/LogisticRegression/LogisticRegression.html#part-c-writing",
    "href": "posts/LogisticRegression/LogisticRegression.html#part-c-writing",
    "title": "Implementing Logistic Regression",
    "section": "Part C: Writing",
    "text": "Part C: Writing\nThrough the process of implementing logistic regression and testing it, I practiced my skills with torch and pandas and learned about the math behind the model. Coding the logistic regression and gradient descent classes helped me practice matrix manipulation and developed my ability to write efficient code. Further, the testing of both vanilla and spicy gradient descent demonstrated the difference that momentum has on gradient descent. Testing the data with more dimensions than data points helped me understand how overfitting occurs and how to prevent it."
  },
  {
    "objectID": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html",
    "href": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html",
    "title": "Newton’s Method of Logistic Regression",
    "section": "",
    "text": "This post explores Newton’s Method for Logistic Regression. I used previous code for creating a linear model and using logistic regression to implement a Newton’s Optimizer class. This class uses the Hessian matrix and gradient of the loss to find the minimal empirical risk of the model. I tested the Newton optimizer in comparison to the gradient descent optimizer, proving that the Newton optimizer converges to the same value as the gradient descent optimizer (and can converge faster than the gradient descent optimizer in some circumstances). I calculated the computational cost of both the Newton optimizer and the gradient descent optimizer and compared the costs of each.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nfrom NewtonOptimizer import LogisticRegression, NewtonOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#abstract",
    "href": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#abstract",
    "title": "Newton’s Method of Logistic Regression",
    "section": "",
    "text": "This post explores Newton’s Method for Logistic Regression. I used previous code for creating a linear model and using logistic regression to implement a Newton’s Optimizer class. This class uses the Hessian matrix and gradient of the loss to find the minimal empirical risk of the model. I tested the Newton optimizer in comparison to the gradient descent optimizer, proving that the Newton optimizer converges to the same value as the gradient descent optimizer (and can converge faster than the gradient descent optimizer in some circumstances). I calculated the computational cost of both the Newton optimizer and the gradient descent optimizer and compared the costs of each.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nfrom NewtonOptimizer import LogisticRegression, NewtonOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#part-a-implementing-newtons-optimizer",
    "href": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#part-a-implementing-newtons-optimizer",
    "title": "Newton’s Method of Logistic Regression",
    "section": "Part A: Implementing Newton’s Optimizer",
    "text": "Part A: Implementing Newton’s Optimizer\nSource code of my implementation of Newton’s Optimizer: https://github.com/tristanmeyer2/tristanmeyer2.github.io/blob/main/posts/NewtMethodLogReg/NewtonOptimizer.py"
  },
  {
    "objectID": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#part-b-perform-experiments",
    "href": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#part-b-perform-experiments",
    "title": "Newton’s Method of Logistic Regression",
    "section": "Part B: Perform Experiments",
    "text": "Part B: Perform Experiments\n\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix]*2-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\ndef converge_loop_grad(LR, opt, X, y, maxIter, alpha, beta): \n    loss = 1\n    loss_vec = []\n    maxIterations = maxIter\n    currIteration = 0\n\n    while loss &gt; 0 and currIteration &lt; maxIterations:\n        loss = LR.loss(X, y) \n        loss_vec.append(loss)\n        opt.step(X, y, alpha = alpha, beta = beta)\n        currIteration += 1\n\n    return loss_vec\n\ndef converge_loop_newt(LR, opt, X, y, maxIter, alpha): \n    loss = 1\n    loss_vec = []\n    maxIterations = maxIter\n    currIteration = 0\n\n    while loss &gt; 0 and currIteration &lt; maxIterations:\n        loss = LR.loss(X, y) \n        loss_vec.append(loss)\n        opt.step(X, y, alpha = alpha)\n        currIteration += 1\n\n    return loss_vec\n\nDefining three methods: plotting data, drawing a line on a visualization, creating data. The plot_data() function plots two dimensional data on a scatter plot The draw_line() function draws a “separating” line that maximizes classification accuracy on two dimensional data.\nThe converge_loop_grad() function uses the logistic regression class and a gradient descent optimizer to find the minimal empirical risk. The converge_loop_newt() function uses the logistic regression class and a Newton’s Method optimizer to find the minimal empirical risk.\n\ntorch.manual_seed(1234)\n\nX, y = classification_data(noise = 0.5)\n\nGenerating data points.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_data(X, y, ax)\n\n\n\n\n\n\n\n\nVisualizing the data points."
  },
  {
    "objectID": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#gradient-descent-performance",
    "href": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#gradient-descent-performance",
    "title": "Newton’s Method of Logistic Regression",
    "section": "Gradient Descent Performance",
    "text": "Gradient Descent Performance\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec_grad = converge_loop_grad(LR, opt, X, y, 100, 0.2, 0.9)\n\nMinimizing empirical risk using the gradient descent optimizer class and using \\(\\alpha\\) = 0.2 and \\(\\beta\\) = 0.9\n\nplt.plot(loss_vec_grad, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_grad)), loss_vec_grad, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThis plot shows the loss over 100 iterations. As shown, the loss decreases to around 0.2; therefore, in order to ensure my implementation of Newton’s Method is working properly, it should reach a loss of around 0.2, as well."
  },
  {
    "objectID": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#newtons-method-performance",
    "href": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#newtons-method-performance",
    "title": "Newton’s Method of Logistic Regression",
    "section": "Newton’s Method Performance",
    "text": "Newton’s Method Performance\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\nloss_vec_newt = converge_loop_newt(LR, opt, X, y, 100, 20)\n\nMinimizing empirical risk using the Newton’s Method optimizer class and using \\(\\alpha\\) = 20.\n\nplt.plot(loss_vec_newt, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_newt)), loss_vec_newt, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAs shown by the plot, the loss decreases to around 0.2 using Newton’s Method.\n\nplt.subplot(1, 2, 1)\nplt.plot(loss_vec_grad, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_grad)), loss_vec_grad, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"loss\", title=\"Gradient Descent\")\n\nplt.subplot(1, 2, 2)\nplt.plot(loss_vec_newt, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_newt)), loss_vec_newt, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"loss\", title=\"Newton's Method\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nAs shown by the comparison between gradient descent and Newton’s method, both optimizers converge to a loss of around 0.2."
  },
  {
    "objectID": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#newtons-method-converging-faster-than-gradient-descent",
    "href": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#newtons-method-converging-faster-than-gradient-descent",
    "title": "Newton’s Method of Logistic Regression",
    "section": "Newton’s Method Converging Faster than Gradient Descent",
    "text": "Newton’s Method Converging Faster than Gradient Descent\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\nloss_vec_newt = converge_loop_newt(LR, opt, X, y, 50, 300)\n\nIf the \\(\\alpha\\) value is increased in our step() function, the optimizer will converge faster. For this experiment, I used \\(\\alpha\\) = 300.\n\nplt.plot(loss_vec_newt, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_newt)), loss_vec_newt, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAs seen by the plot, the optimizer converged in around 4 iterations.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec_grad = converge_loop_grad(LR, opt, X, y, 50, 0.2, 0.9)\n\nplt.subplot(1, 2, 1)\nplt.plot(loss_vec_grad, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_grad)), loss_vec_grad, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"loss\", title=\"Gradient Descent\")\n\nplt.subplot(1, 2, 2)\nplt.plot(loss_vec_newt, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_newt)), loss_vec_newt, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"loss\", title=\"Newton's Method\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nAs shown by the comparison of the charts, Newton’s method can converge faster than spicy gradient descent."
  },
  {
    "objectID": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#newtons-method-failing-to-converge",
    "href": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#newtons-method-failing-to-converge",
    "title": "Newton’s Method of Logistic Regression",
    "section": "Newton’s Method Failing to Converge",
    "text": "Newton’s Method Failing to Converge\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\nloss_vec_newt = converge_loop_newt(LR, opt, X, y, 100, 601)\n\nUsing \\(\\alpha\\) = 601 as a learning rate for Newton’s method.\n\nplt.plot(loss_vec_newt, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_newt)), loss_vec_newt, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAs shown in the graph of the loss through 100 iterations, Newton’s method fails to converge at a loss of 0.2 and instead begins to increase at around 30 iterations and seems to converge around 0.3 over 100 iterations. Therefore, using \\(\\alpha\\) = 601, Newton’s method does not perform correctly."
  },
  {
    "objectID": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#part-c-operation-counting",
    "href": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#part-c-operation-counting",
    "title": "Newton’s Method of Logistic Regression",
    "section": "Part C: Operation Counting",
    "text": "Part C: Operation Counting\nGiven the following computational times:\nLoss = \\(c\\)\nGradient = \\(2c\\)\nHessian = \\(pc\\)\nInverting a pxp matrix = \\(k_{1}p^{\\gamma}\\)\nMatrix-Vector multiplication = \\(k_{2}p^{2}\\)\nGiven Newton’s Method converges in \\(t_{nm}\\) steps and gradient descent converges in \\(t_{gd}\\) steps\nComputational Cost of Gradient Descent = \\(t_{gd}(2c)+c\\)\nComputational Cost of Newton’s Method = \\(t_{nm}(2c+pc+k_{1}p^{\\gamma}+k_{2}p^{2})+c\\)\nComputing how much smaller \\(t_{nm}\\) must be than \\(t_{gd}\\) to ensure Newton’s Method will require fewer computational units to complete:\n\\[t_{nm}(2c+pc+k_{1}p^{\\gamma}+k_{2}p^{2})+c &lt; t_{gd}(2c)+c\\] \\[t_{nm}(2c+pc+k_{1}p^{\\gamma}+k_{2}p^{2}) &lt; t_{gd}(2c)\\] \\[t_{nm}(1+\\frac{p}{2}+\\frac{k_{1}p^{\\gamma}}{2c}+\\frac{k_{2}p^{2}}{2c}) &lt; t_{gd}\\]\nThis inequality shows that in order for Newton’s method to have a smaller computational cost that gradient descent, \\(t_{nm}\\) have to be smaller than \\(t_{gd}\\) by a factor of \\((1+\\frac{p}{2}+\\frac{k_{1}p^{\\gamma}}{2c}+\\frac{k_{2}p^{2}}{2c})\\)\nFurther, when the number of features, p, becomes large, it’s unlikely that Newton’s Method will ever be computationally more efficient than gradient descent."
  },
  {
    "objectID": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#conclusions",
    "href": "posts/NewtMethodLogReg/NewtonsMethodLogReg.html#conclusions",
    "title": "Newton’s Method of Logistic Regression",
    "section": "Conclusions",
    "text": "Conclusions\nI successfully implemented Newton’s Method for logistic regression and tested it under multiple circumstances. This post was great practice for implementing efficient mathematical equations in python. I also improved my ability to using pyplot to display my experience. This post was also a great refresher on calculating the computational cost of functions and comparing two costs."
  },
  {
    "objectID": "posts/ImplementingPerceptron/ImplementingPerceptron.html",
    "href": "posts/ImplementingPerceptron/ImplementingPerceptron.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nimport torch\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/ImplementingPerceptron/ImplementingPerceptron.html#abstract",
    "href": "posts/ImplementingPerceptron/ImplementingPerceptron.html#abstract",
    "title": "Implementing Perceptron",
    "section": "Abstract",
    "text": "Abstract\nThis blog post explores the perceptron algorithm. I implement the algorithm in python, then test my implementation over different sets of data. I test the algorithm over data that is linearly separable and data that is not, and data of different dimensions. I also implemented a minibatch perceptron algorithm that considers multiple points each iteration. I tested this implementation over different batch sizes using different sets of data."
  },
  {
    "objectID": "posts/ImplementingPerceptron/ImplementingPerceptron.html#part-a-implementing-perceptron",
    "href": "posts/ImplementingPerceptron/ImplementingPerceptron.html#part-a-implementing-perceptron",
    "title": "Implementing Perceptron",
    "section": "Part A: Implementing Perceptron",
    "text": "Part A: Implementing Perceptron\nLink to perceptron source code: https://github.com/tristanmeyer2/tristanmeyer2.github.io/blob/main/posts/ImplementingPerceptron/perceptron.py\nLink to minibatch perceptron source code: https://github.com/tristanmeyer2/tristanmeyer2.github.io/blob/main/posts/ImplementingPerceptron/minibatchperceptron.py\nHere is my implementation for the grad() function:\ndef grad(self, X, y):\n    s_i = self.score(X) \n    return torch.where(y*s_i &lt; 0, y@X, 0.0)\nThe function calculates the score using matrix multiplication with the models weight vector, then returns a tensor in which if the score multiplied by the y is less than 0, the value is y matrix multiplied by X, and if it is greater than 0, the value is zero."
  },
  {
    "objectID": "posts/ImplementingPerceptron/ImplementingPerceptron.html#checking-implementation",
    "href": "posts/ImplementingPerceptron/ImplementingPerceptron.html#checking-implementation",
    "title": "Implementing Perceptron",
    "section": "Checking Implementation",
    "text": "Checking Implementation\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points, noise, p_dims):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n    y = y.type(torch.FloatTensor)\n\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nCreating a perceptron_data function, which generates data over two quantitative variables and a qualitative feature. Creating a plot_perceptron_data function which visualizes the data created by the perceptron_data function. Creating a draw_line function which visualizes a line on a plot.\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.3, p_dims = 2)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nThe perceptron data is split into an X feature matrix and a y target vector. The plot shows the linearly separable perceptron data.\n\ntorch.manual_seed(1234)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nThe perceptron algorithm is run in a minimal training loop that calls the loss function and the step function until there is zero loss (no misclassifications).\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThis plot shows the loss over several iterations of the perceptron algorithm. As shown, the loss decreases until it reaches a value of zero."
  },
  {
    "objectID": "posts/ImplementingPerceptron/ImplementingPerceptron.html#part-b-experiments",
    "href": "posts/ImplementingPerceptron/ImplementingPerceptron.html#part-b-experiments",
    "title": "Implementing Perceptron",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments"
  },
  {
    "objectID": "posts/ImplementingPerceptron/ImplementingPerceptron.html#using-2d-linearly-separable-data",
    "href": "posts/ImplementingPerceptron/ImplementingPerceptron.html#using-2d-linearly-separable-data",
    "title": "Implementing Perceptron",
    "section": "Using 2D linearly separable data:",
    "text": "Using 2D linearly separable data:\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.3, p_dims = 2)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nGenerating and visualization linearly separable data.\n\ntorch.manual_seed(1234567)\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5) \nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\nmaxIterations = 1000\ncurrIteration = 0\n\nwhile loss &gt; 0 and currIteration &lt; maxIterations:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    randomPoint = torch.randint(X.shape[0], size = (1,))\n    X_i = X[[randomPoint],:]\n    y_i = y[randomPoint]\n    local_loss = opt.step(X_i, y_i)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[randomPoint,0],X[randomPoint,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[randomPoint].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n    currIteration += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe perceptron algorithm is run on a training loop that terminates after 1000 iterations. The changes in the loss value are visualized on the plot, showing the previous estimate of a separating line (shown by the dashed line), and the change that the perceptron optimizer made to the separating line (shown by the solid line). The plot shows the final iteration, which resulted in a separating line with zero loss.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThis plot shows the changes in the loss over different iterations of the perceptron algorithm.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nThis plot shows the final iteration of the perceptron algorithm, which resulted in a loss of zero. As shown, the line perfectly separates the data."
  },
  {
    "objectID": "posts/ImplementingPerceptron/ImplementingPerceptron.html#using-2d-non-linearly-separable-data",
    "href": "posts/ImplementingPerceptron/ImplementingPerceptron.html#using-2d-non-linearly-separable-data",
    "title": "Implementing Perceptron",
    "section": "Using 2D non-linearly separable data:",
    "text": "Using 2D non-linearly separable data:\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.5, p_dims = 2)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nThe data shown above is not linearly separable.\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmaxIterations = 1000\ncurrIteration = 0\n\nwhile loss &gt; 0 and currIteration &lt; maxIterations: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    currIteration += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nOver a thousand iterations, the perceptron algorithm never results in a loss of zero because the data is not linearly separable. However, the algorithm did result in a loss of under 0.1.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nThe separating line above appears to effectively separate the dataset while minimizing the loss (there are only a few points that are misclassified)."
  },
  {
    "objectID": "posts/ImplementingPerceptron/ImplementingPerceptron.html#using-5d-linearly-separable-data",
    "href": "posts/ImplementingPerceptron/ImplementingPerceptron.html#using-5d-linearly-separable-data",
    "title": "Implementing Perceptron",
    "section": "Using 5D linearly separable data:",
    "text": "Using 5D linearly separable data:\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.1, p_dims = 5)\n\nGenerating data over five dimensions.\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmaxIterations = 1000\ncurrIteration = 0\n\nwhile loss &gt; 0 and currIteration &lt; maxIterations: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    currIteration += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAs shown by the graph of the loss vector, the algorithm reaches a loss of zero in under 25 iterations, even with five dimensional data."
  },
  {
    "objectID": "posts/ImplementingPerceptron/ImplementingPerceptron.html#part-c-minibatch-perceptron",
    "href": "posts/ImplementingPerceptron/ImplementingPerceptron.html#part-c-minibatch-perceptron",
    "title": "Implementing Perceptron",
    "section": "Part C: Minibatch Perceptron",
    "text": "Part C: Minibatch Perceptron\n\nfrom minibatchperceptron import MiniBatchPerceptron, MiniBatchPerceptronOptimizer\n\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.22, p_dims = 2)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nGenerating two dimensional linearly separable data.\n\ndef training_loop(k, a, p, opt):\n    \n    loss = 1.0\n\n    # for keeping track of loss values\n    loss_vec = []\n\n    n = X.size()[0]\n    maxIterations = 10000\n    currIteration = 0\n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n\n    while loss &gt; 0 and currIteration &lt; maxIterations: \n        \n        # not part of the update: just for tracking our progress    \n        loss = p.loss(X, y) \n        loss_vec.append(loss)\n        \n        # pick a random group of data points\n        ix = torch.randperm(X.size(0))[:k]\n        x_ix = X[ix,:]\n        y_ix = y[ix]\n        \n        # perform a perceptron update using the random data point\n        opt.step(x_ix, y_ix, a, k)\n        currIteration += 1\n    return loss_vec\n\nDefining a new training loop for the minibatch perceptron algorithm.\n\np = MiniBatchPerceptron() \nopt = MiniBatchPerceptronOptimizer(p)\nloss_vec = training_loop(1,1, p, opt)\n\nTesting the minibatch algorithm with a batch size of 1 and alpha of 1.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nThis chart shows the separating line produced by this algorithm.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAs shown by the chart of the loss vector, the algorithm reached a loss of zero in three iterations.\n\np = MiniBatchPerceptron() \nopt = MiniBatchPerceptronOptimizer(p)\nloss_vec = training_loop(10,3, p, opt)\n\nTesting the minibatch algorithm with a batch size of 10 and alpha of 3.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nThis chart shows the separating line produced by this algorithm.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAs shown by the chart of the loss vector, the algorithm reached a loss of zero in around 10,000 iterations.\n\ntorch.manual_seed(1235)\nX, y = perceptron_data(n_points = 50, noise = 0.5, p_dims = 2)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nGenerating non-linearly separable data over two dimensions.\n\np = MiniBatchPerceptron() \nopt = MiniBatchPerceptronOptimizer(p)\nloss_vec = training_loop(50,0.01, p, opt)\n\nTesting the minibatch algorithm with a batch size equal to the number of points and alpha of 0.01.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nDue to the large batch size, the algorithm does not perform perfectly. The line separating the data does not minimize the loss.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThis charts shows the loss over 10,000 iterations of the algorithm."
  },
  {
    "objectID": "posts/ImplementingPerceptron/ImplementingPerceptron.html#runtime-analysis",
    "href": "posts/ImplementingPerceptron/ImplementingPerceptron.html#runtime-analysis",
    "title": "Implementing Perceptron",
    "section": "Runtime Analysis",
    "text": "Runtime Analysis\nThe runtime of a single iteration of the perceptron algorithm with n points and p features is O(p). The runtime is O(p) because for each iteration, the algorithm checks the misclassification of a single data point over p features using matrix multiplication, therefore if the number of features increases, the algorithm has to do more computation.\nThe runtime of a single iteration of the mini batch algorithm with n points, p features, and a batch size of b is O(p * b). The runtime is O(p * b) because this runtime can be thought of as doing the regular perceptron grad function b times, therefore as the batch size increases, the algorithm needs to do more computation."
  },
  {
    "objectID": "posts/ImplementingPerceptron/ImplementingPerceptron.html#conclusion",
    "href": "posts/ImplementingPerceptron/ImplementingPerceptron.html#conclusion",
    "title": "Implementing Perceptron",
    "section": "Conclusion",
    "text": "Conclusion\nI implemented the perceptron and minibatch perceptron in this blog post. I explored each algorithm with different data sets with different dimensions. This post was great practice working with matrix multiplication and assuring the dimensions of my matrices were correct. This post was also good practice working with training loops and displaying the results from my tests."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Optimal Decision Making\n\n\n\n\n\nBuilding a model to maximize gain for a bank given past loan data\n\n\n\n\n\nMar 21, 2024\n\n\nTristan Meyer\n\n\n\n\n\n\n\n\n\n\n\n\nNewton’s Method of Logistic Regression\n\n\n\n\n\nImplementing and testing Newton’s Method of logistic regression \n\n\n\n\n\nMay 14, 2023\n\n\nTristan Meyer\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nImplementing and testing logistic regression with different gradient descent strategies\n\n\n\n\n\nApr 28, 2023\n\n\nTristan Meyer\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\nImplementing and testing the perceptron and minibatch perceptron algorithms\n\n\n\n\n\nApr 7, 2023\n\n\nTristan Meyer\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nPredicting Penguin Species\n\n\n\n\n\nMar 4, 2023\n\n\nTristan Meyer\n\n\n\n\n\n\nNo matching items"
  }
]