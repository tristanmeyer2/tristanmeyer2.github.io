[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog Test"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Second Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\nTristan Meyer\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "ClassifyingPalmerPenguins.html",
    "href": "ClassifyingPalmerPenguins.html",
    "title": "Explore:",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "ClassifyingPalmerPenguins.html#explore",
    "href": "ClassifyingPalmerPenguins.html#explore",
    "title": "Explore:",
    "section": "Explore:",
    "text": "Explore:\nPlease create two interesting visualizations of the data, and one summary table (e.g. compute the average or median value of some features, by group). Your visualizations and table should:\n\nInclude axis labels and legends.\nHelp you draw conclusions about what features you are going to try using for your model.\nBe accompanied by discussion of what you learned and how you will use it in modeling. Most figures and tables are worth at least one short paragraph of discussion.\n\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ndf1 = train[[\"Sex\",\"Body Mass (g)\", \"Species\"]]\ndf1 = df1.drop(df1[df1[\"Sex\"] == \".\"].index)\ndf1[\"Species\"] = df1[\"Species\"].str.split().str.get(0)\ndf1 = df1.dropna()\n\np1 = sns.boxplot(df1, x = \"Sex\", y = \"Body Mass (g)\", hue = \"Species\").set_title(\"Comparing Penguins' Body Mass and Sex across Species\")\n\n\n\n\n\n\n\n\nDESCRIBE SOMETHING\n\ndf2 = train[[\"Body Mass (g)\", \"Flipper Length (mm)\"]]\ndf2 = df2.dropna()\n\np2 = sns.scatterplot(df2, x = \"Body Mass (g)\", y = \"Flipper Length (mm)\", color = \"blue\").set_title(\"Comparing Flipper Length and Body Mass of Penguins\")\n\n\n\n\n\n\n\n\nDESCRIBE SOMETHING\n\ndf3 = train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island\"]]\ndf3 = df3.dropna()\n\np3 = sns.scatterplot(df3, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue=\"Island\").set_title(\"Comparing Culmen Length and Culmen Depth across Islands\")\n\n\n\n\n\n\n\n\nDESCRIBE SOMETHING\n\ndf4 = train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Sex\"]]\ndf4.groupby(\"Sex\").aggregate('mean')\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\nSex\n\n\n\n\n\n\n\n\n.\n44.500000\n15.700000\n217.000000\n4875.000000\n\n\nFEMALE\n42.551128\n16.371429\n198.067669\n3882.330827\n\n\nMALE\n45.463359\n17.993130\n203.687023\n4521.755725\n\n\n\n\n\n\n\nWrite something"
  },
  {
    "objectID": "ClassifyingPalmerPenguins.html#model",
    "href": "ClassifyingPalmerPenguins.html#model",
    "title": "Explore:",
    "section": "Model:",
    "text": "Model:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns\n\n\n\nPreparing the qualitative columns in the dataset and removing the Species column\nWrite Something\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Stage_Adult, 1 Egg Stage\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nmax_score = 0\nmax_cols = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression(max_iter=10000)\n    LR.fit(X_train[cols], y_train)\n    score = LR.score(X_train[cols], y_train)\n    if score &gt; max_score:\n      max_score = score\n      max_cols = cols\nmax_cols\n\n['Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen',\n 'Culmen Length (mm)',\n 'Culmen Depth (mm)']\n\n\nWrite Something\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\nmax_cols.reverse()\n\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[max_cols], y_train)\nLR.score(X_test[max_cols], y_test)\n\n1.0\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[max_cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test[max_cols], y_test)\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[max_cols])\nC = confusion_matrix(y_test,y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])"
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#abstract",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this post was to create a model that predicts a penguin’s species based on both quantitative and qualitative data. Personally, this post demonstrates my ability to work with and manipulate dataframes in Pandas, create visualizations using seaborn, choose effective features, and build an effective model. My model uses Culmen Depth, Culmen Length and the island of inhabitants of the penguins in the Palmer Penguin data set to predict the species of the penguin. I used a brute force tactic to choose these three features: iterating through every combination of features and fitting a logistic regression to each combination. I compared the mean cross validation scores for each combination and chose the three features with the highest score. I displayed the results of my model on both the test and training data, showing the prediction areas produced by the features."
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#explore",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#explore",
    "title": "Classifying Palmer Penguins",
    "section": "Explore:",
    "text": "Explore:\nPlease create two interesting visualizations of the data, and one summary table (e.g. compute the average or median value of some features, by group). Your visualizations and table should:\n\nInclude axis labels and legends.\nHelp you draw conclusions about what features you are going to try using for your model.\nBe accompanied by discussion of what you learned and how you will use it in modeling. Most figures and tables are worth at least one short paragraph of discussion.\n\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nImporting the Palmer Penguin Dataset.\n\nimport seaborn as sns\n\ndf1 = train[[\"Sex\",\"Body Mass (g)\", \"Species\"]]\ndf1 = df1.drop(df1[df1[\"Sex\"] == \".\"].index)\ndf1[\"Species\"] = df1[\"Species\"].str.split().str.get(0)\ndf1 = df1.dropna()\n\np1 = sns.boxplot(df1, x = \"Sex\", y = \"Body Mass (g)\", hue = \"Species\").set_title(\"Comparing Penguins' Body Mass and Sex across Species\")\n\n\n\n\n\n\n\n\nI created a new data frame with the the Sex, Body Mass, and Species columns. I removed the penguins with the third sex (not male or female) because there were not enough specimens with this sex to make an impact on this visualization. Further, I dropped all of the rows with NA values for any of these features. I then created a boxplot with Sex on the X-axis and Body Mass on the Y-axis with the individual boxplots separated by species. This plot shows that the average body mass of all species is greater for male penguins compared to females. Further, (and important for feature selection) it shows the similarities between body mass between the Chinstrap and Adelie species.\n\ndf2 = train[[\"Body Mass (g)\", \"Flipper Length (mm)\"]]\ndf2 = df2.dropna()\n\np2 = sns.scatterplot(df2, x = \"Body Mass (g)\", y = \"Flipper Length (mm)\", color = \"blue\").set_title(\"Comparing Flipper Length and Body Mass of Penguins\")\n\n\n\n\n\n\n\n\nI created another data frame (dropping the NA values again) including the body mass and flipper length features. I displayed these features using a scatter plot. This visualization shows that there is a positive correlation between body mass and flipper length among all the penguins in the data.\n\ndf3 = train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island\"]]\ndf3 = df3.dropna()\n\np3 = sns.scatterplot(df3, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue=\"Island\").set_title(\"Comparing Culmen Length and Culmen Depth across Islands\")\n\n\n\n\n\n\n\n\nFor my final visualization, I created another data frame that included the culmen length, culmen depth, and island features. This visualization shows that particular ranges of culmen length and depth could be associated with different islands. For example, generally, penguins with culmen depth under 16 mm and culmen length over 43 mm belong to the Biscoe island.\n\ndf4 = train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Sex\"]]\ndf4.groupby(\"Sex\").aggregate('mean')\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\nSex\n\n\n\n\n\n\n\n\n.\n44.500000\n15.700000\n217.000000\n4875.000000\n\n\nFEMALE\n42.551128\n16.371429\n198.067669\n3882.330827\n\n\nMALE\n45.463359\n17.993130\n203.687023\n4521.755725\n\n\n\n\n\n\n\nI created this summary table by grouping the penguins and sex and aggregating the culmen length, culmen depth, flipper length, and body mass features by their mean values. Through this table, the differences in these quantitative values between sexes."
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#model",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#model",
    "title": "Classifying Palmer Penguins",
    "section": "Model:",
    "text": "Model:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nPreparing the qualitative columns in the dataset and removing the Species column. I also removed the other identifying columns for the penguins and removed the third sex (not female or male) because there are a small number of penguins with this sex. I also one hot encoded the categorical feature columns.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Stage_Adult, 1 Egg Stage\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nmax_cv_score = 0\nmax_cols = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression(max_iter=10000)\n    LR.fit(X_train[cols], y_train)\n    cv_scores = cross_val_score(LR,X_train[cols],y_train,cv = 10)\n    cv_score_mean = cv_scores.mean()\n    if cv_score_mean &gt; max_cv_score:\n      max_cv_score = cv_score_mean\n      max_cols = cols\nmax_cols\n\n['Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen',\n 'Culmen Length (mm)',\n 'Culmen Depth (mm)']\n\n\nI split the features into the quantitative columns and the qualitative columns. Next, for every qualitative feature, I iterated through every combination of two quantitative features. For each of these combinations of one qualitative and two quantitative features, I fit a logistic regression model and calculated the mean cross evaluation score over ten folds. I kept track of the greatest mean cross evaluation score and found that Culmen Length, Culmen Depth, and Island performed best.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\nmax_cols.reverse()\n\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[max_cols], y_train)\nLR.score(X_test[max_cols], y_test)\n\n1.0\n\n\nI imported the test data, and fit a logistic regression on the training data with the columns that performed the best in the training data. I then scored the model using the testing data and found that it performed to 100% accuracy.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[max_cols], y_train)\n\n\n\n\n\n\n\n\nThis plot shows the prediction regions for my logistic regression model. As shown in the plot, the model uses culmen length and depth and island of inhabitants to make it’s predictions. The training data is shown in this visualization\n\nplot_regions(LR, X_test[max_cols], y_test)\n\n\n\n\n\n\n\n\nThe plot above uses the test data instead of the training data over the same variables. As shown in the plot, the model performed with one hundred percent accuracy for the test data.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[max_cols])\nC = confusion_matrix(y_test,y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nThe above confusion matrix shows that all of the Adelie penguins were classified as Adelie penguins, all of the Chinstrap penguins were classified as Chinstrap penguins, and all of the Gentoo penguins were classified as Gentoo."
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this post was to create a model that predicts a penguin’s species based on both quantitative and qualitative data. Personally, this post demonstrates my ability to work with and manipulate dataframes in Pandas, create visualizations using seaborn, choose effective features, and build an effective model. My model uses Culmen Depth, Culmen Length and the island of inhabitants of the penguins in the Palmer Penguin data set to predict the species of the penguin. I used a brute force tactic to choose these three features: iterating through every combination of features and fitting a logistic regression to each combination. I compared the mean cross validation scores for each combination and chose the three features with the highest score. I displayed the results of my model on both the test and training data, showing the prediction areas produced by the features."
  },
  {
    "objectID": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#discussion",
    "href": "posts/PalmerPenguins/ClassifyingPalmerPenguins.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion:",
    "text": "Discussion:\nThrough the use of an exhaustive feature search and cross validation, I created a model that uses culmen depth, culmen length, and the island of inhabitants to predict the species of the penguin. My model performed with 99.6 percent accuracy in the training data with a cross validation score of 99.6 percent as well, and performed with one hundred percent accuracy in the test data.\nThrough this process of completing this process I improved my skills with pandas data frames and fitting linear regression models. I also learned about feature selection. I experimented with multiple tools in the scikit learn database (VarianceThreshold, SelectKBest, etc.), but I didn’t end up using any of these tools for my selection. I learned how to test every combination of features and practiced my skills with cross validation on each of these combinations."
  }
]